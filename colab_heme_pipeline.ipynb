{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# ðŸ§¬ Heme Binder Diffusion Pipeline for Google Colab\n",
    "\n",
    "This notebook provides a simplified version of the heme binding protein design pipeline optimized for Google Colab Pro.\n",
    "\n",
    "## âš ï¸ Important Notes:\n",
    "- **GPU Required**: Make sure you have GPU enabled (Runtime > Change runtime type > GPU)\n",
    "- **Time Limits**: Colab Pro has 24h session limits - save progress frequently\n",
    "- **Storage**: Use Google Drive for persistent storage\n",
    "- **Resources**: Monitor GPU/RAM usage to avoid session termination\n",
    "\n",
    "## ðŸš€ Pipeline Overview:\n",
    "1. **Setup Environment** - Install dependencies and download models\n",
    "2. **Generate Backbones** - Use RFdiffusion to create protein scaffolds\n",
    "3. **Design Sequences** - Use ProteinMPNN for sequence design\n",
    "4. **Predict Structures** - Use AlphaFold2 for structure prediction\n",
    "5. **Optimize Binding** - Use LigandMPNN for binding site optimization\n",
    "6. **Analyze Results** - Score and filter final designs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup_header"
   },
   "source": [
    "## ðŸ”§ Step 1: Environment Setup\n",
    "\n",
    "This will take 15-30 minutes depending on your internet connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check_gpu"
   },
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"âœ… GPU detected: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "else:\n",
    "    print(\"âŒ No GPU detected! Please enable GPU in Runtime > Change runtime type\")\n",
    "    raise RuntimeError(\"GPU required for this pipeline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mount_drive"
   },
   "outputs": [],
   "source": [
    "# Mount Google Drive for persistent storage\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create project directory in Drive\n",
    "import os\n",
    "DRIVE_DIR = \"/content/drive/MyDrive/heme_binder_diffusion\"\n",
    "os.makedirs(DRIVE_DIR, exist_ok=True)\n",
    "print(f\"âœ… Project directory: {DRIVE_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clone_repo"
   },
   "outputs": [],
   "source": [
    "# Clone repository and setup\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "# Clone the repository\n",
    "if not os.path.exists(\"/content/heme_binder_diffusion\"):\n",
    "    !git clone https://github.com/ikalvet/heme_binder_diffusion.git /content/heme_binder_diffusion\n",
    "    \n",
    "os.chdir(\"/content/heme_binder_diffusion\")\n",
    "\n",
    "# Initialize submodules\n",
    "!git submodule init\n",
    "!git submodule update\n",
    "\n",
    "print(\"âœ… Repository cloned and submodules initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run_setup"
   },
   "outputs": [],
   "source": [
    "# Download and run setup script\n",
    "import requests\n",
    "import os\n",
    "\n",
    "# Download setup script (you'll need to upload this to a accessible URL)\n",
    "setup_script_url = \"https://raw.githubusercontent.com/YOUR_USERNAME/heme_binder_diffusion/main/colab_setup.py\"\n",
    "\n",
    "# For now, we'll create the script inline\n",
    "setup_script = '''\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Install core dependencies\n",
    "print(\"Installing core dependencies...\")\n",
    "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"--upgrade\", \"pip\"])\n",
    "\n",
    "# Core packages\n",
    "packages = [\n",
    "    \"torch>=1.12.0\",\n",
    "    \"numpy>=1.21.0\",\n",
    "    \"pandas>=1.3.0\",\n",
    "    \"matplotlib>=3.5.0\",\n",
    "    \"seaborn\",\n",
    "    \"biopython>=1.79\",\n",
    "    \"prody>=2.0\",\n",
    "    \"py3dmol>=1.8\",\n",
    "    \"tqdm\",\n",
    "    \"requests\"\n",
    "]\n",
    "\n",
    "for package in packages:\n",
    "    try:\n",
    "        subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package], check=True)\n",
    "        print(f\"âœ… {package}\")\n",
    "    except:\n",
    "        print(f\"âŒ {package}\")\n",
    "\n",
    "# Install JAX\n",
    "try:\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"jax[cuda12_pip]\", \"-f\", \n",
    "                   \"https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\"], check=True)\n",
    "    print(\"âœ… JAX with CUDA\")\n",
    "except:\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"jax\", \"jaxlib\"])\n",
    "    print(\"âœ… JAX (CPU version)\")\n",
    "\n",
    "print(\"âœ… Setup completed!\")\n",
    "'''\n",
    "\n",
    "# Write and execute setup script\n",
    "with open('colab_setup.py', 'w') as f:\n",
    "    f.write(setup_script)\n",
    "\n",
    "exec(open('colab_setup.py').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download_models"
   },
   "outputs": [],
   "source": [
    "# Download essential models (this will take time!)\n",
    "import os\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "def download_file(url, filepath):\n",
    "    response = requests.get(url, stream=True)\n",
    "    total_size = int(response.headers.get('content-length', 0))\n",
    "    \n",
    "    with open(filepath, 'wb') as f:\n",
    "        with tqdm(total=total_size, unit='B', unit_scale=True, desc=f\"Downloading {os.path.basename(filepath)}\") as pbar:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "                    pbar.update(len(chunk))\n",
    "\n",
    "# Create models directory\n",
    "models_dir = \"/content/models\"\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "# Download ProteinMPNN models (smaller and essential)\n",
    "print(\"ðŸ“¥ Downloading ProteinMPNN models...\")\n",
    "mpnn_dir = f\"{models_dir}/proteinmpnn\"\n",
    "os.makedirs(mpnn_dir, exist_ok=True)\n",
    "\n",
    "# Note: You'll need to check these URLs and adjust them\n",
    "models_to_download = {\n",
    "    \"proteinmpnn_v_48_020.pt\": \"https://files.ipd.uw.edu/pub/training_sets/proteinmpnn_v_48_020.pt\",\n",
    "    \"ligandmpnn_v_32_010_25.pt\": \"https://files.ipd.uw.edu/pub/training_sets/ligandmpnn_v_32_010_25.pt\"\n",
    "}\n",
    "\n",
    "for filename, url in models_to_download.items():\n",
    "    filepath = f\"{mpnn_dir}/{filename}\"\n",
    "    if not os.path.exists(filepath):\n",
    "        try:\n",
    "            download_file(url, filepath)\n",
    "            print(f\"âœ… {filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Failed to download {filename}: {e}\")\n",
    "    else:\n",
    "        print(f\"âœ… {filename} already exists\")\n",
    "\n",
    "print(\"âœ… Essential models downloaded!\")\n",
    "print(\"âš ï¸ Note: AlphaFold2 and RFdiffusion models are very large (~4GB each)\")\n",
    "print(\"   They will be downloaded on-demand when needed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "config_header"
   },
   "source": [
    "## âš™ï¸ Configuration and Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "config_setup"
   },
   "outputs": [],
   "source": [
    "# Setup configuration\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# Paths\n",
    "BASE_DIR = \"/content/heme_binder_diffusion\"\n",
    "MODELS_DIR = \"/content/models\"\n",
    "WORK_DIR = \"/content/work\"\n",
    "DRIVE_DIR = \"/content/drive/MyDrive/heme_binder_diffusion\"\n",
    "\n",
    "# Create working directory\n",
    "os.makedirs(WORK_DIR, exist_ok=True)\n",
    "\n",
    "# Add scripts to Python path\n",
    "sys.path.append(f\"{BASE_DIR}/scripts/utils\")\n",
    "sys.path.append(f\"{BASE_DIR}/lib/LigandMPNN\")\n",
    "\n",
    "# Project settings\n",
    "PROJECT_NAME = \"colab_heme_design\"\n",
    "LIGAND = \"HBA\"\n",
    "N_DESIGNS = 3  # Reduced for Colab\n",
    "T_STEPS = 100  # Reduced for Colab\n",
    "\n",
    "print(f\"âœ… Configuration set up\")\n",
    "print(f\"ðŸ“ Base directory: {BASE_DIR}\")\n",
    "print(f\"ðŸ“ Work directory: {WORK_DIR}\")\n",
    "print(f\"ðŸ“ Drive backup: {DRIVE_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "helper_functions"
   },
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "import subprocess\n",
    "import time\n",
    "import json\n",
    "from IPython.display import HTML, display\n",
    "import py3dmol\n",
    "\n",
    "def run_command(cmd, description=\"\"):\n",
    "    \"\"\"Run a shell command and return result\"\"\"\n",
    "    print(f\"ðŸ”„ {description}\")\n",
    "    print(f\"Command: {cmd}\")\n",
    "    \n",
    "    result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(f\"âœ… {description} completed\")\n",
    "        return result.stdout\n",
    "    else:\n",
    "        print(f\"âŒ {description} failed\")\n",
    "        print(f\"Error: {result.stderr}\")\n",
    "        return None\n",
    "\n",
    "def save_to_drive(local_path, drive_subdir=\"\"):\n",
    "    \"\"\"Save files to Google Drive\"\"\"\n",
    "    drive_path = f\"{DRIVE_DIR}/{drive_subdir}\"\n",
    "    os.makedirs(drive_path, exist_ok=True)\n",
    "    \n",
    "    if os.path.isfile(local_path):\n",
    "        shutil.copy2(local_path, drive_path)\n",
    "    else:\n",
    "        shutil.copytree(local_path, f\"{drive_path}/{os.path.basename(local_path)}\", dirs_exist_ok=True)\n",
    "    \n",
    "    print(f\"ðŸ’¾ Saved to Drive: {drive_path}\")\n",
    "\n",
    "def visualize_pdb(pdb_path, width=400, height=400):\n",
    "    \"\"\"Visualize PDB structure\"\"\"\n",
    "    with open(pdb_path, 'r') as f:\n",
    "        pdb_data = f.read()\n",
    "    \n",
    "    view = py3dmol.view(width=width, height=height)\n",
    "    view.addModel(pdb_data, 'pdb')\n",
    "    view.setStyle({'cartoon': {'color': 'spectrum'}})\n",
    "    view.setStyle({'hetflag': True}, {'stick': {'colorscheme': 'greenCarbon'}})\n",
    "    view.zoomTo()\n",
    "    view.show()\n",
    "    \n",
    "    return view\n",
    "\n",
    "def monitor_resources():\n",
    "    \"\"\"Monitor GPU and RAM usage\"\"\"\n",
    "    import torch\n",
    "    import psutil\n",
    "    \n",
    "    # GPU usage\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "        gpu_used = torch.cuda.memory_allocated(0) / 1024**3\n",
    "        gpu_percent = (gpu_used / gpu_memory) * 100\n",
    "        print(f\"ðŸ–¥ï¸  GPU: {gpu_used:.1f}/{gpu_memory:.1f} GB ({gpu_percent:.1f}%)\")\n",
    "    \n",
    "    # RAM usage\n",
    "    ram = psutil.virtual_memory()\n",
    "    ram_used = ram.used / 1024**3\n",
    "    ram_total = ram.total / 1024**3\n",
    "    ram_percent = ram.percent\n",
    "    print(f\"ðŸ’¾ RAM: {ram_used:.1f}/{ram_total:.1f} GB ({ram_percent:.1f}%)\")\n",
    "    \n",
    "    # Disk usage\n",
    "    disk = psutil.disk_usage('/')\n",
    "    disk_used = disk.used / 1024**3\n",
    "    disk_total = disk.total / 1024**3\n",
    "    disk_percent = (disk_used / disk_total) * 100\n",
    "    print(f\"ðŸ’¿ Disk: {disk_used:.1f}/{disk_total:.1f} GB ({disk_percent:.1f}%)\")\n",
    "\n",
    "print(\"âœ… Helper functions loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pipeline_header"
   },
   "source": [
    "## ðŸ§¬ Step 2: Simplified Pipeline Execution\n",
    "\n",
    "This is a streamlined version that focuses on the core functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check_resources"
   },
   "outputs": [],
   "source": [
    "# Check resources before starting\n",
    "print(\"ðŸ“Š Current resource usage:\")\n",
    "monitor_resources()\n",
    "\n",
    "# Check available input files\n",
    "input_files = glob.glob(f\"{BASE_DIR}/input/*.pdb\")\n",
    "print(f\"\\nðŸ“ Found {len(input_files)} input PDB files:\")\n",
    "for f in input_files:\n",
    "    print(f\"  - {os.path.basename(f)}\")\n",
    "\n",
    "if len(input_files) == 0:\n",
    "    print(\"âš ï¸ No input files found! Please check the input directory.\")\n",
    "else:\n",
    "    print(\"âœ… Ready to start pipeline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "visualize_input"
   },
   "outputs": [],
   "source": [
    "# Visualize input structure\n",
    "if input_files:\n",
    "    input_pdb = input_files[0]  # Use first file as example\n",
    "    print(f\"ðŸ“Š Visualizing input structure: {os.path.basename(input_pdb)}\")\n",
    "    \n",
    "    view = visualize_pdb(input_pdb)\n",
    "    \n",
    "    # Also show some basic statistics\n",
    "    with open(input_pdb, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    atom_lines = [l for l in lines if l.startswith('ATOM')]\n",
    "    hetatm_lines = [l for l in lines if l.startswith('HETATM')]\n",
    "    \n",
    "    print(f\"ðŸ“ˆ Structure statistics:\")\n",
    "    print(f\"  - Protein atoms: {len(atom_lines)}\")\n",
    "    print(f\"  - Ligand atoms: {len(hetatm_lines)}\")\n",
    "    print(f\"  - Total lines: {len(lines)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mpnn_header"
   },
   "source": [
    "### ðŸ§© Step 2.1: ProteinMPNN Sequence Design\n",
    "\n",
    "We'll start with sequence design using ProteinMPNN (skipping RFdiffusion for now due to model size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run_proteinmpnn"
   },
   "outputs": [],
   "source": [
    "# Setup ProteinMPNN run\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Create output directory\n",
    "mpnn_dir = f\"{WORK_DIR}/proteinmpnn\"\n",
    "os.makedirs(mpnn_dir, exist_ok=True)\n",
    "os.chdir(mpnn_dir)\n",
    "\n",
    "# ProteinMPNN parameters\n",
    "temperatures = [0.1, 0.2, 0.3]\n",
    "n_sequences = 3  # Reduced for Colab\n",
    "omit_AAs = \"CM\"\n",
    "\n",
    "# Create a simple mask (no fixed residues for this demo)\n",
    "mask_dict = {}\n",
    "for input_file in input_files:\n",
    "    basename = os.path.basename(input_file).replace('.pdb', '')\n",
    "    mask_dict[basename] = {}\n",
    "\n",
    "with open('mask.jsonl', 'w') as f:\n",
    "    json.dump(mask_dict, f)\n",
    "\n",
    "print(f\"âœ… ProteinMPNN setup complete\")\n",
    "print(f\"ðŸ“Š Will generate {len(temperatures) * n_sequences} sequences per structure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "execute_proteinmpnn"
   },
   "outputs": [],
   "source": [
    "# Run ProteinMPNN\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Check if ProteinMPNN script exists\n",
    "mpnn_script = f\"{BASE_DIR}/lib/LigandMPNN/run.py\"\n",
    "if not os.path.exists(mpnn_script):\n",
    "    print(f\"âŒ ProteinMPNN script not found: {mpnn_script}\")\n",
    "    print(\"Please make sure submodules are properly initialized\")\n",
    "else:\n",
    "    print(f\"âœ… ProteinMPNN script found\")\n",
    "    \n",
    "    # Run for each temperature and input file\n",
    "    for temp in temperatures:\n",
    "        for input_file in input_files[:1]:  # Only first file for demo\n",
    "            cmd = f\"\"\"{sys.executable} {mpnn_script} \\\n",
    "                --model_type protein_mpnn \\\n",
    "                --ligand_mpnn_use_atom_context 0 \\\n",
    "                --fixed_residues_multi mask.jsonl \\\n",
    "                --out_folder ./ \\\n",
    "                --number_of_batches {n_sequences} \\\n",
    "                --temperature {temp} \\\n",
    "                --omit_AA {omit_AAs} \\\n",
    "                --pdb_path {input_file} \\\n",
    "                --checkpoint_protein_mpnn {MODELS_DIR}/proteinmpnn/proteinmpnn_v_48_020.pt\"\"\"\n",
    "            \n",
    "            print(f\"ðŸ”„ Running ProteinMPNN with temperature {temp}...\")\n",
    "            result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n",
    "            \n",
    "            if result.returncode == 0:\n",
    "                print(f\"âœ… Completed T={temp}\")\n",
    "            else:\n",
    "                print(f\"âŒ Failed T={temp}\")\n",
    "                print(f\"Error: {result.stderr}\")\n",
    "                \n",
    "    # Check outputs\n",
    "    fasta_files = glob.glob(\"seqs/*.fa\")\n",
    "    print(f\"\\nðŸ“Š Generated {len(fasta_files)} FASTA files\")\n",
    "    \n",
    "    if fasta_files:\n",
    "        # Save to Drive\n",
    "        save_to_drive(mpnn_dir, \"proteinmpnn_results\")\n",
    "        print(\"âœ… ProteinMPNN results saved to Drive\")\n",
    "    else:\n",
    "        print(\"âš ï¸ No FASTA files generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "af2_header"
   },
   "source": [
    "### ðŸ”¬ Step 2.2: AlphaFold2 Structure Prediction\n",
    "\n",
    "We'll use ColabFold for faster AlphaFold2 predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_colabfold"
   },
   "outputs": [],
   "source": [
    "# Install ColabFold if not already installed\n",
    "try:\n",
    "    import colabfold\n",
    "    print(\"âœ… ColabFold already installed\")\n",
    "except ImportError:\n",
    "    print(\"ðŸ“¦ Installing ColabFold...\")\n",
    "    !pip install -q colabfold[alphafold]\n",
    "    print(\"âœ… ColabFold installed\")\n",
    "\n",
    "# Alternative: Use the original AlphaFold2 script if available\n",
    "af2_script = f\"{BASE_DIR}/scripts/af2/af2.py\"\n",
    "if os.path.exists(af2_script):\n",
    "    print(\"âœ… Original AlphaFold2 script found\")\n",
    "    use_original_af2 = True\n",
    "else:\n",
    "    print(\"âš ï¸ Using ColabFold instead of original AlphaFold2\")\n",
    "    use_original_af2 = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "prepare_af2"
   },
   "outputs": [],
   "source": [
    "# Prepare sequences for AlphaFold2\n",
    "import os\n",
    "from Bio import SeqIO\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "from Bio.Seq import Seq\n",
    "\n",
    "# Create AF2 directory\n",
    "af2_dir = f\"{WORK_DIR}/alphafold2\"\n",
    "os.makedirs(af2_dir, exist_ok=True)\n",
    "os.chdir(af2_dir)\n",
    "\n",
    "# Parse ProteinMPNN outputs\n",
    "sequences = {}\n",
    "fasta_files = glob.glob(f\"{mpnn_dir}/seqs/*.fa\")\n",
    "\n",
    "for fasta_file in fasta_files:\n",
    "    with open(fasta_file, 'r') as f:\n",
    "        for record in SeqIO.parse(f, 'fasta'):\n",
    "            seq_id = f\"{os.path.basename(fasta_file).replace('.fa', '')}_{record.id}\"\n",
    "            sequences[seq_id] = str(record.seq)\n",
    "\n",
    "print(f\"ðŸ“Š Found {len(sequences)} sequences for AlphaFold2 prediction\")\n",
    "\n",
    "# Create combined FASTA file\n",
    "records = []\n",
    "for seq_id, seq in sequences.items():\n",
    "    records.append(SeqRecord(Seq(seq), id=seq_id, description=\"\"))\n",
    "\n",
    "with open('input_sequences.fasta', 'w') as f:\n",
    "    SeqIO.write(records, f, 'fasta')\n",
    "\n",
    "print(f\"âœ… Created input_sequences.fasta with {len(records)} sequences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run_af2"
   },
   "outputs": [],
   "source": [
    "# Run AlphaFold2 predictions\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "if use_original_af2:\n",
    "    # Use original AlphaFold2 script\n",
    "    cmd = f\"\"\"{sys.executable} {af2_script} \\\n",
    "        --af-nrecycles 3 \\\n",
    "        --af-models 4 \\\n",
    "        --fasta input_sequences.fasta \\\n",
    "        --scorefile af2_scores.csv\"\"\"\n",
    "    \n",
    "    print(\"ðŸ”„ Running AlphaFold2 predictions...\")\n",
    "    print(\"â° This may take 10-30 minutes depending on sequence length and number\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(f\"âœ… AlphaFold2 completed in {(end_time - start_time)/60:.1f} minutes\")\n",
    "    else:\n",
    "        print(f\"âŒ AlphaFold2 failed\")\n",
    "        print(f\"Error: {result.stderr}\")\n",
    "        \n",
    "else:\n",
    "    # Use ColabFold\n",
    "    print(\"ðŸ”„ Running ColabFold predictions...\")\n",
    "    print(\"â° This may take 5-15 minutes\")\n",
    "    \n",
    "    # ColabFold command\n",
    "    cmd = f\"colabfold_batch input_sequences.fasta ./predictions --num-models 1 --num-recycles 3\"\n",
    "    \n",
    "    start_time = time.time()\n",
    "    result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(f\"âœ… ColabFold completed in {(end_time - start_time)/60:.1f} minutes\")\n",
    "    else:\n",
    "        print(f\"âŒ ColabFold failed\")\n",
    "        print(f\"Error: {result.stderr}\")\n",
    "\n",
    "# Check outputs\n",
    "pdb_files = glob.glob(\"*.pdb\") + glob.glob(\"predictions/*.pdb\")\n",
    "print(f\"\\nðŸ“Š Generated {len(pdb_files)} PDB files\")\n",
    "\n",
    "if pdb_files:\n",
    "    # Save to Drive\n",
    "    save_to_drive(af2_dir, \"alphafold2_results\")\n",
    "    print(\"âœ… AlphaFold2 results saved to Drive\")\n",
    "else:\n",
    "    print(\"âš ï¸ No PDB files generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "analysis_header"
   },
   "source": [
    "## ðŸ“Š Step 3: Analysis and Visualization\n",
    "\n",
    "Analyze the results and visualize the best structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "analyze_results"
   },
   "outputs": [],
   "source": [
    "# Analyze AlphaFold2 results\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load confidence scores if available\n",
    "confidence_files = glob.glob(\"*_confidence.json\") + glob.glob(\"predictions/*_confidence.json\")\n",
    "pdb_files = glob.glob(\"*.pdb\") + glob.glob(\"predictions/*.pdb\")\n",
    "\n",
    "print(f\"ðŸ“Š Analyzing {len(pdb_files)} structures\")\n",
    "\n",
    "# Basic analysis\n",
    "results = []\n",
    "for pdb_file in pdb_files:\n",
    "    # Get basic info\n",
    "    basename = os.path.basename(pdb_file)\n",
    "    \n",
    "    # Count atoms\n",
    "    with open(pdb_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    atom_lines = [l for l in lines if l.startswith('ATOM')]\n",
    "    \n",
    "    # Extract confidence scores if available (ColabFold format)\n",
    "    confidence_scores = []\n",
    "    for line in atom_lines:\n",
    "        if len(line) > 60:\n",
    "            try:\n",
    "                confidence = float(line[60:66].strip())\n",
    "                confidence_scores.append(confidence)\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    avg_confidence = np.mean(confidence_scores) if confidence_scores else 0\n",
    "    \n",
    "    results.append({\n",
    "        'filename': basename,\n",
    "        'n_atoms': len(atom_lines),\n",
    "        'avg_confidence': avg_confidence,\n",
    "        'path': pdb_file\n",
    "    })\n",
    "\n",
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "if len(results_df) > 0:\n",
    "    print(\"\\nðŸ“ˆ Results Summary:\")\n",
    "    print(results_df.describe())\n",
    "    \n",
    "    # Plot confidence scores\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(results_df['avg_confidence'], bins=20, alpha=0.7)\n",
    "    plt.xlabel('Average Confidence Score')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Distribution of Confidence Scores')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.scatter(results_df['n_atoms'], results_df['avg_confidence'], alpha=0.7)\n",
    "    plt.xlabel('Number of Atoms')\n",
    "    plt.ylabel('Average Confidence Score')\n",
    "    plt.title('Confidence vs Structure Size')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Save results\n",
    "    results_df.to_csv('analysis_results.csv', index=False)\n",
    "    print(\"âœ… Analysis results saved\")\n",
    "else:\n",
    "    print(\"âš ï¸ No structures to analyze\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "visualize_best"
   },
   "outputs": [],
   "source": [
    "# Visualize best structures\n",
    "if len(results_df) > 0:\n",
    "    # Sort by confidence score\n",
    "    best_structures = results_df.sort_values('avg_confidence', ascending=False).head(3)\n",
    "    \n",
    "    print(\"ðŸ† Top 3 structures by confidence:\")\n",
    "    print(best_structures[['filename', 'avg_confidence', 'n_atoms']])\n",
    "    \n",
    "    # Visualize the best structure\n",
    "    best_pdb = best_structures.iloc[0]['path']\n",
    "    print(f\"\\nðŸ“Š Visualizing best structure: {os.path.basename(best_pdb)}\")\n",
    "    \n",
    "    view = visualize_pdb(best_pdb, width=600, height=400)\n",
    "    \n",
    "    # Show structure info\n",
    "    print(f\"\\nðŸ“ˆ Structure Details:\")\n",
    "    print(f\"  - Filename: {best_structures.iloc[0]['filename']}\")\n",
    "    print(f\"  - Confidence: {best_structures.iloc[0]['avg_confidence']:.2f}\")\n",
    "    print(f\"  - Atoms: {best_structures.iloc[0]['n_atoms']}\")\n",
    "else:\n",
    "    print(\"âš ï¸ No structures available for visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "save_results_header"
   },
   "source": [
    "## ðŸ’¾ Step 4: Save Results and Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save_all_results"
   },
   "outputs": [],
   "source": [
    "# Save all results to Google Drive\n",
    "import shutil\n",
    "import zipfile\n",
    "from datetime import datetime\n",
    "\n",
    "# Create timestamped results directory\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "results_dir = f\"{DRIVE_DIR}/results_{timestamp}\"\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "# Copy all important files\n",
    "files_to_save = {\n",
    "    'proteinmpnn_results': f\"{WORK_DIR}/proteinmpnn\",\n",
    "    'alphafold2_results': f\"{WORK_DIR}/alphafold2\",\n",
    "    'analysis_results.csv': 'analysis_results.csv'\n",
    "}\n",
    "\n",
    "for name, path in files_to_save.items():\n",
    "    if os.path.exists(path):\n",
    "        dest_path = f\"{results_dir}/{name}\"\n",
    "        if os.path.isdir(path):\n",
    "            shutil.copytree(path, dest_path, dirs_exist_ok=True)\n",
    "        else:\n",
    "            shutil.copy2(path, dest_path)\n",
    "        print(f\"âœ… Saved {name}\")\n",
    "\n",
    "# Create a summary report\n",
    "summary_report = f\"\"\"\n",
    "# Heme Binder Diffusion Pipeline Results\n",
    "Generated: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\n",
    "\n",
    "## Pipeline Summary\n",
    "- Project: {PROJECT_NAME}\n",
    "- Ligand: {LIGAND}\n",
    "- Input files: {len(input_files)}\n",
    "- Generated sequences: {len(sequences)}\n",
    "- Predicted structures: {len(pdb_files)}\n",
    "\n",
    "## Best Results\n",
    "\"\"\";\n",
    "\n",
    "if len(results_df) > 0:\n",
    "    for idx, row in best_structures.iterrows():\n",
    "        summary_report += f\"\\n- {row['filename']}: Confidence {row['avg_confidence']:.2f}\"\n",
    "\n",
    "summary_report += f\"\"\"\n",
    "\n",
    "## Files Generated\n",
    "- ProteinMPNN sequences: {len(fasta_files)} FASTA files\n",
    "- AlphaFold2 structures: {len(pdb_files)} PDB files\n",
    "- Analysis results: analysis_results.csv\n",
    "\n",
    "## Next Steps\n",
    "1. Review the top-confidence structures\n",
    "2. Perform binding site optimization with LigandMPNN\n",
    "3. Run molecular dynamics simulations\n",
    "4. Experimental validation\n",
    "\"\"\"\n",
    "\n",
    "with open(f\"{results_dir}/README.md\", 'w') as f:\n",
    "    f.write(summary_report)\n",
    "\n",
    "print(f\"\\nâœ… All results saved to: {results_dir}\")\n",
    "print(f\"ðŸ“Š Summary report created: {results_dir}/README.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "final_cleanup"
   },
   "outputs": [],
   "source": [
    "# Final resource check and cleanup\n",
    "print(\"ðŸ§¹ Final cleanup and resource check:\")\n",
    "monitor_resources()\n",
    "\n",
    "# Optional: Clean up large temporary files\n",
    "cleanup_choice = input(\"\\nClean up temporary files? (y/n): \")\n",
    "if cleanup_choice.lower() == 'y':\n",
    "    import shutil\n",
    "    \n",
    "    # Remove work directory (keeping only Drive backup)\n",
    "    if os.path.exists(WORK_DIR):\n",
    "        shutil.rmtree(WORK_DIR)\n",
    "        print(f\"ðŸ§¹ Cleaned up {WORK_DIR}\")\n",
    "    \n",
    "    # Clear GPU cache\n",
    "    import torch\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"ðŸ§¹ Cleared GPU cache\")\n",
    "    \n",
    "    monitor_resources()\n",
    "\n",
    "print(\"\\nðŸŽ‰ Pipeline completed successfully!\")\n",
    "print(f\"ðŸ“ Results saved in Google Drive: {results_dir}\")\n",
    "print(\"\\nðŸ“‹ Summary:\")\n",
    "print(f\"  - Input structures: {len(input_files)}\")\n",
    "print(f\"  - Generated sequences: {len(sequences)}\")\n",
    "print(f\"  - Predicted structures: {len(pdb_files)}\")\n",
    "if len(results_df) > 0:\n",
    "    print(f\"  - Best confidence score: {results_df['avg_confidence'].max():.2f}\")\n",
    "    print(f\"  - Average confidence: {results_df['avg_confidence'].mean():.2f}\")\n",
    "    \n",
    "print(\"\\nðŸš€ Next steps:\")\n",
    "print(\"  1. Download results from Google Drive\")\n",
    "print(\"  2. Analyze structures in detail\")\n",
    "print(\"  3. Run binding site optimization\")\n",
    "print(\"  4. Prepare for experimental validation\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}